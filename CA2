# MSc Data Analytics CA2
## Data Visualisation and Preparation

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import math

## Data Extraction and Manipulation
df_UN = pd.read_csv("UNDataset.csv")

df_UN.head()

df_UN["City"].unique()

df_UN.sort_values(["City"])

newdf = df_UN.loc[(df_UN["City"] == 'Dublin') | 
                  (df_UN["City"] == 'Porto') | 
                  (df_UN["City"] == 'Glasgow') | 
                  (df_UN["City"] == 'Manchester')| 
                  (df_UN["City"] == 'Edinburgh')| 
                  (df_UN["City"] == 'Copenhagen')|
                 (df_UN["City"] == 'Zurich')]
newdf

newdf["City"].unique()

newdf1 = newdf.loc[(newdf["Type"] == 'Tram')|
                 (newdf["Type"] == 'Metro')]
newdf1

newdf2 = newdf1.loc[(newdf1["Variable"] == 'Pass')]

newdf3 = newdf2.loc[(newdf2["Year"] >= 2018)]

newdf3.shape

df_final=newdf3.drop(columns=['Countrycode','Type', 'Variable', 'Note'])

df_final.head()

df_final= df_final.rename(columns={"Value (Thousands of Passengers)":"Annual No. of Passengers (Thousand)"})
df_final.head()

df_final.shape

df_final.sort_values(["City"])

df_final.shape

df_final2 = pd.read_csv("ManualData.csv")

df_final2.sort_values(["City"])

df_final2

mergedf1 = pd.merge(df_final, df_final2, on = ["City", "Year"])
mergedf1.head()

mergedf1

mergedf1.set_index('Year')

## Exploratory Data Analysis
mergedf1.describe(include=object)
mergedf1.describe()

mergedf1["City"].unique()

duplicate_rows_df = mergedf1[mergedf1.duplicated()] 
print("Number of duplicate rows: ", duplicate_rows_df.shape)

print(mergedf1.isnull().sum())

## use of fillna() to take average of previous four years for each city to fill null value with mean of previous four years. Source: https://stackoverflow.com/questions/58507317/how-to-fillna-impute-by-using-the-mean-of-the-last-3-rows-in-the-same-column

mergedf1.fillna(mergedf1.rolling(4, min_periods=1).mean().shift()) 

## using a boxplot to gain insight into the data 

sns.boxplot(x=mergedf1['Annual No. of Passengers (Thousand)']) 

plt.title("Annual No. of Passengers Across Seven European Cities", fontsize=14)
plt.xlabel("No. of Passengers (Thousand)", fontsize=12)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Annual No. of Passengers (Thousand)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Annual Number of Passengers Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Population (thousand)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Population Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Congestion (%)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Congestion Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

sns.scatterplot(data=mergedf1, x="Annual No. of Passengers (Thousand)", y="Congestion (%)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Annual Number of Passengers vs Congestion (%)", fontsize=14)
plt.xlabel("Annual No. of Passengers (Thousand)", fontsize=10)
plt.ylabel("Congestion (%)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Price of One Litre of Diesel (Euro)" , y="Congestion (%)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Price of One Litre of Diesel (Euro) vs Congestion (%)", fontsize=14)
plt.xlabel("Price of One Litre of Diesel (Euro)", fontsize=10)
plt.ylabel("Congestion (%)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Annual No. of Passengers (Thousand)", y="Population (thousand)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Annual No. of Passengers (Thousand) vs Population (thousand)", fontsize=14)
plt.xlabel("Annual No. of Passengers (Thousand)", fontsize=10)
plt.ylabel("Population (thousand)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Congestion (%)", y="Population (thousand)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Congestion (%) vs Population (thousand)", fontsize=14)
plt.xlabel("Congestion (%)", fontsize=10)
plt.ylabel("Population (thousand)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

mergedf1.corr()

## plotting data to gain insights into the data set

sns.relplot(data=mergedf1, x="Year", y="Annual No. of Passengers (Thousand)", hue="City", kind="line", height=8)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-") ## Source https://www.geeksforgeeks.org/grids-in-matplotlib/

plt.title("Annual No. of Passengers Across Seven European Cities", fontsize=14)
plt.xlabel("Year", fontsize=12)
plt.ylabel("Annual No. of Passengers (thousand)", fontsize=12)

sns.histplot(data=mergedf1, x="Congestion (%)", kde=True)

sns.histplot(data=mergedf1, x="Price of One Litre of Diesel (Euro)", kde=True)

# Statistical Analysis

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import statistics as stats
import scipy.stats as stats
from statsmodels.stats import weightstats

### Descriptive Statistics
mergedf.describe()

sns.pairplot(mergedf)
plt.grid(True, color = "grey", linewidth = "1", linestyle = "-") ## Source https://www.geeksforgeeks.org/grids-in-matplotlib/
plt.title("Graphs of Various Variables Effecting Congestion in Cities in Europe", fontsize=14)

sns.histplot(data=mergedf, x="Congestion (%)", kde=True)

sns.histplot(data=mergedf, x="Annual No. of Passengers (Thousand)", kde=True)

sns.histplot(data=mergedf, x="Population", kde=True)

plt.title("Histogram Showing Population of Seven European Cities 2018-2022", fontsize=14)

### Inferential Statistics

## Hypothesis Test (Z-Test)
X = mergedf["Annual No. of Passengers (Thousand)"]
passenger_std = X.std()
#### H0: mu = 58071, H1: mu > 58071 : As the mean of the annual number of passengers is 58071 the alternative hypothesis should be greater than 5000.
mu = 58071
z_score, p_value = weightstats.ztest(X, value = mu, ddof=1, alternative = 'larger')
z_score
p_value
As the p-value is greater than alpha, we reject the null hypothesis and accept the alternative hypothesis that mu > 58071. 
Therefore, we have enough evidence at 95% confidence interval to suggest that the average annual number of passengers will be greater than 58,071,000 across the seven cities.
X = mergedf["Congestion"]
congestion_std1 = X.std()
## H0: mu = 29.42, H1: mu > 29.42 : As the mean of the annual number of passengers is 58071 the alternative hypothesis should be greater than 5000.
mu = 29.42
z_score, p_value = weightstats.ztest(X, value = mu, ddof=1, alternative = 'larger')
z_score
p_value
As the p-value is greater than alpha we accept the null hypothesis and reject the alternative hypothesis that mu > 29.42 
Therefore, we have enough evidence at 95% confidence interval to suggest that the average congestion level across the seven cities will be 29.42

## Confidence Intervals
## define x (variable to analyse)- Annual No. Of Passengers
X = mergedf.iloc[:,2:3].values
## create our confidence interval using Student T's at 95% confidence interval
stats.t.interval(confidence = 0.95, df=len(X), loc=np.mean(X), scale = stats.sem(X))
At 95% confidence level, the annual number of passengers in the seven countries considered will be between 36438.70 and 79704.77 thousand people. 

## define x (variable to analyse)- Congestion (%)
X1 = mergedf.iloc[:,3:4].values
## create our confidence interval using Student T's at 95% confidence interval
stats.t.interval(confidence = 0.95, df=len(X1), loc=np.mean(X1), scale = stats.sem(X1))
At 95% confidence level, the average congestion level across the seven countries considered will be between 26.75% and 32.11% people.

## Normal Distribution
from scipy import stats
stats.norm.interval(0.95, loc=np.mean(X), scale=stats.sem(X))
Values for normal distribution are very similar to that of the Student T's

# Parametric Testing
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
import scipy as scipy

## Correlation Analysis
corr_main = mergedf.drop(columns=["City","Year"])
corr_main.corr()
corr = corr_main.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Variables from Seven European Cities", fontsize=18, loc="center")

mergedf = mergedf.rename(columns={"Annual No. of Passengers (Thousand)":"Passengers", "Population (thousand)":"Population", "Congestion (%)":"Congestion"})
pivot1new = df1.pivot(index="Year", columns="City", values="Passengers")
pivot1new
pivot1new.corr()
corr = pivot1new.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Annual No. of Passengers from Seven European Cities", fontsize=18, loc="center")

pivot2 = mergedf.drop(columns=["Passengers", "Population", "Price of One Litre of Diesel (Euro)"])
pivot2new = pivot2.pivot(index="Year", columns="City", values="Congestion")
pivot2new
pivot2new.corr()
corr = pivot2new.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Congestion Levels from Seven European Cities", fontsize=18, loc="center")

## ANOVA 
### Sharpiro Wilk Test- Checking the conditions for ANOVA
#QQ-plot
stats.probplot(mergedf.Passengers, plot = plt)
plt.figure()
## Sharpiro Wil Test to test Normality of Data
## H0: Data comes from normal dist
## H1: Data does NOT come from normal dist
stats.shapiro(mergedf.Passengers[mergedf.City=="Dublin"])
## p_value is greater than alpha therefore null hypothesis should be accepted.
## Evidence suggests that data for Dublin comes from normal distribution
stats.shapiro(mergedf.Passengers[mergedf.City=="Copenhagen"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Porto"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Zurich"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Glasgow"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Edinburgh"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Manchester"])
## As evidenced above by plot and p_value test, data for Zurich is NOT normally distributed

#QQ-plot
stats.probplot(mergedf.Congestion, plot = plt)
plt.figure()
stats.shapiro(mergedf.Congestion[mergedf.City=="Dublin"])
## p_value is greater than alpha therefore null hypothesis should be accepted.
## Evidence suggests that data for Dublin comes from normal distribution
stats.shapiro(mergedf.Congestion[mergedf.City=="Copenhagen"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Porto"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Zurich"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Manchester"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Edinburgh"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Glasgow"])
### Preparing the variables
dublin = mergedf.Congestion[mergedf.City=="Dublin"]
copenhagen = mergedf.Congestion[mergedf.City=="Copenhagen"]
porto = mergedf.Congestion[mergedf.City=="Porto"]
zurich = mergedf.Congestion[mergedf.City=="Zurich"]
manchester = mergedf.Congestion[mergedf.City=="Manchester"]
glasgow = mergedf.Congestion[mergedf.City=="Glasgow"]
edinburgh = mergedf.Congestion[mergedf.City=="Edinburgh"]

## Homogenity of the variances ---> Levene Test
## H0: the variances are equal
## H1: the variances are not equal
from scipy.stats import levene
## Levene Test for Homogeinety
levene(dublin, copenhagen, porto, zurich, edinburgh, manchester, glasgow, center = 'mean')
## H0: muDublin = muCopenhgen = muZurich = muPorto = muManchester = muGlasgow = muEdinburgh
## H1: there are at least 2 mu which are different

## we analyse congestion based on city
## ANOVA (One Way)
model = ols('Congestion~City', data=mergedf).fit()
aov = sm.stats.anova_lm(model, type=2)
print(aov)
## therefore as PR(>F) value which is our p_value is less than alpha then we reject the null hypothesis.

## ANOVA (Two Way)
## H0: means are equal
## H1: at least one mean is different
model2 = ols('Congestion~Year+City', data = mergedf).fit()
aov2 = sm.stats.anova_lm(model2, type = 2)
print(aov2)
## therefore as PR(>F) value which is our p_value is less than alpha then we reject the null hypothesis.
## Means are not equal based on City and Year of Congestion Data.

# Non-Parametric Tests
### Kruskal Wallis Test
new_mergedf_pivot
## Step One: Hypothesis

## H0: there is no difference between the numbers of passengers in each city
## H1: there is at least one city that has a different level of passengers

## Step Two: Create Variables
C= pivot1new["Copenhagen"]
D= pivot1new["Dublin"]
E= pivot1new["Edinburgh"]
G= pivot1new["Glasgow"]
M= pivot1new["Manchester"]
P= pivot1new["Porto"]
Z= pivot1new["Zurich"]
from scipy.stats import kruskal
stat, p = kruskal(C,D,E,G,M,P,Z)
print("Statistical Test: ", stat)
print("p-value= ", p)
## Therefore, p-value is less than alpha so we reject the null hypothesis.
## Evidence suggests that there is at least one city that has a different level of passenger

## Step One: Hypothesis
## H0: there is no difference between the congestion level in each city
## H1: there is at least one city that has a different congestion level
## Step Two: Create Variables
C1= pivot2new["Copenhagen"]
D1= pivot2new["Dublin"]
E1= pivot2new["Edinburgh"]
G1= pivot2new["Glasgow"]
M1= pivot2new["Manchester"]
P1= pivot2new["Porto"]
Z1= pivot2new["Zurich"]
stat1, p1 = kruskal(C1,D1,E1,G1,M1,P1,Z1)
print("Statistical Test: ", stat1)
print("p-value= ", p1)
## Therefore, p-value is less than alpha so we reject the null hypothesis.
## Evidence suggests that there is at least one city that has a different level of congestion

## U-Mann Whitman Test
##Step One: Hypothesis
## H0: there is no difference between the passenger numbers in cities
## H1: there is a difference between the passenger numbers in cities
## Step Two: Creating Variables
## Step 3: Test
from scipy.stats import mannwhitneyu
stat, p = mannwhitneyu(D,C)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,P)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,Z)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,G)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,E)
print ("Statistical Test: ", stat)
print("p-value: ", p)

##Step One: Hypothesis

## H0: there is no difference between the congestion levels in cities
## H1: there is a difference between the congestion levels in cities
## Step Two: Creating Variables
## Step 3: Test
stat, p = mannwhitneyu(D1,C1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,M1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,G1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,E1)
print ("Statistical Test: ", stat)
print("p-value: ", p)

# Machine Learning
## Linear Regression
## one hot encoding to help with preparing data for machine learning source: https://www.analyticsvidhya.com/blog/2021/05/how-to-perform-one-hot-encoding-for-multi-categorical-variables/
for x in newdf.columns:
#print unique values
print(x ,':', len(newdf[x].unique()))
top7 = [x for x in newdf.City.value_counts().sort_values(ascending=False).head(10).index]
top7
for label in top7:
newdf[label] = np.where(newdf["City"]==label,1,0)
newdf[["City"]+top7]
## pivot2 to have Year as index City as column head, and congestion as values
pivot_table = pivot2.pivot_table(index='Year', columns='City', values='Congestion', aggfunc='mean')
## pivot table to have cities as columns and years as index
pivot_new_year= pivot_table.T
## display new table
print(pivot_new_year)

# Congestion Data
## Extract the features (X) and target (y)
x = pivot2[['City']].values
y = pivot2[['Congestion']].values
## Display independent and dependent variables
print(x.shape, y.shape)
## import the libraries for LinearRegression
from sklearn.model_selection import train_test_split
## Call the train_test_split method to split the data and the splitting is 70% for training and 30% for testing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 98)
x.shape, y.shape, x_train.shape, x_test.shape, y_train.shape, y_test.shape
## import libaray
from sklearn.linear_model import LinearRegression
## train the LinearRegression mode by using a method fit() function
regression = LinearRegression().fit(x_train_encoded, y_train) ## use of encoded as completed previously
# Display the coefficients
print(regression.coef_)
## get scores for test and training sets
print("Training set score: {:.2f}".format(regression.score(x_train_encoded, y_train)))
print("Test set score: {:.2f}".format(regression.score(x_test_encoded, y_test)))
# Calculate the mean square error
mean_squared_error = np.mean((regression_predict - y)**2)
# Display the mean square error
print("Mean Squared Error on test set", mean_squared_error)

#Passenger Data
pivot1 = mergedf.drop(columns=["Congestion", "Population", "Price of One Litre of Diesel (Euro)"])

## Extract the features (X) and target (y)
x = pivot1[['City']].values
y = pivot1[['Congestion']].values
## Display independent and dependent variables
print(x.shape, y.shape)
## import the libraries for LinearRegression
from sklearn.model_selection import train_test_split
## Call the train_test_split method to split the data and the splitting is 70% for training and 30% for testing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 98)
x.shape, y.shape, x_train.shape, x_test.shape, y_train.shape, y_test.shape
## import libaray
from sklearn.linear_model import LinearRegression
## train the LinearRegression mode by using a method fit() function
regression = LinearRegression().fit(x_train_encoded, y_train) ## use of encoded as completed previously
# Display the coefficients
print(regression.coef_)
## get scores for test and training sets
print("Training set score: {:.2f}".format(regression.score(x_train_encoded, y_train)))
print("Test set score: {:.2f}".format(regression.score(x_test_encoded, y_test)))
# Calculate the mean square error
mean_squared_error = np.mean((regression_predict - y)**2)
# Display the mean square error
print("Mean Squared Error on test set", mean_squared_error)

## kNN Regressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error 
sns.scatterplot(x=mergedf['Congestion'], y=mergedf['Passengers'], hue=mergedf["City"])
# Create the independent and dependent variables
x = mergedf[['Congestion']]
y = mergedf[['Passengers']]
from sklearn.model_selection import train_test_split

# Split the data into train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3,random_state = 0)

from sklearn.neighbors import KNeighborsRegressor
# Declare arrays to store training and test accuracies
neighbors = np.arange(1, 9)
train_accuracy =np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
for i, k in enumerate(neighbors):
    # Declare and initialise kNN regressor with k neighbors
    kNN = KNeighborsRegressor(n_neighbors = k)
    # Call the method fit() to train the model
    kNN.fit(x_train, y_train)
    # Compute accuracy on the training set
    train_accuracy[i] = kNN.score(x_train, y_train)
    # Compute accuracy on the test set
    test_accuracy[i] = kNN.score(x_test, y_test) 
# Visualise the accuracy based on the number of neighbors
plt.title('kNN considering different neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.show()

print("Training set score: {:.2f}".format(kNN.score(x_train, y_train)))
print("Test set score: {:.2f}".format(kNN.score(x_test, y_test)))

## calculating MSE Source: https://www.datatechnotes.com/2019/04/regression-example-with-k-nearest.html
pred_y = kNN.predict(x)
mse = mean_squared_error(y, pred_y)
print("Mean Squared Error:",mse)

## Principal Component Analysis
# import the libraries for the cancer datasert
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
# Importing standardscalar module 
from sklearn.preprocessing import StandardScaler
scalar = StandardScaler()
# fitting
scalar.fit(df)
scaled_data = scalar.transform(df) 
# Importing PCA
from sklearn.decomposition import PCA
# Let's say, components = 2
pca = PCA(n_components = 2)
pca.fit(scaled_data)
x_pca = pca.transform(scaled_data)
x_pca.shape
# giving a larger plot
plt.figure(figsize =(8, 6)
plt.scatter(x_pca[:, 0], x_pca[:, 1], c = df['Congestion'], cmap ='plasma')
# labeling x and y axes
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')

## Decision Tree Model
# split dataset in features and target variable
feature_cols = ['Year', 'Passengers', 'Congestion','Price of One Litre of Diesel (Euro)','Population']
X = mergedf[feature_cols] # Features
y = mergedf.City # Target variable
X, y
### Splitting Data
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test
### Building Decision Tree Model
# Create Decision Tree classifer object
clf = DecisionTreeClassifier(max_depth = 3, random_state = 0)
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
### Evaluating Model
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# column names for dataset
fn=['Year', 'Passengers', 'Congestion','Price of One Litre of Diesel (Euro)','Population']
# classes of dataset
cn=['0','1','2','3','4','5','6']
# Setting dpi = 300 to make image clearer than default
fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)
tree.plot_tree(clf,
           feature_names = fn, 
           class_names=cn,
           filled = True);
# In case of any errors, install conda install python-graphviz     on the command line
### Optimizing Decision Tree Performance
# Create Decision Tree classifer object
clf = DecisionTreeClassifier(criterion="entropy", max_depth=3)
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
# column names for dataset
fn=['Year', 'Passengers', 'Congestion','Price of One Litre of Diesel (Euro)','Population']
# classes of dataset
cn=['0','1','2','3','4','5','6']
# Setting dpi = 300 to make image clearer than default
fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)
tree.plot_tree(clf,
           feature_names = fn, 
           class_names=cn,
           filled = True);
# set the path of your system
fig.savefig('imagename.png',bbox_inches='tight')



