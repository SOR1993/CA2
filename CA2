# MSc Data Analytics CA2
## Data Visualisation and Preparation

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import math

## Data Extraction and Manipulation
df_UN = pd.read_csv("UNDataset.csv")

df_UN.head()

df_UN["City"].unique()

df_UN.sort_values(["City"])

newdf = df_UN.loc[(df_UN["City"] == 'Dublin') | 
                  (df_UN["City"] == 'Porto') | 
                  (df_UN["City"] == 'Glasgow') | 
                  (df_UN["City"] == 'Manchester')| 
                  (df_UN["City"] == 'Edinburgh')| 
                  (df_UN["City"] == 'Copenhagen')|
                 (df_UN["City"] == 'Zurich')]
newdf

newdf["City"].unique()

newdf1 = newdf.loc[(newdf["Type"] == 'Tram')|
                 (newdf["Type"] == 'Metro')]
newdf1

newdf2 = newdf1.loc[(newdf1["Variable"] == 'Pass')]

newdf3 = newdf2.loc[(newdf2["Year"] >= 2018)]

newdf3.shape

df_final=newdf3.drop(columns=['Countrycode','Type', 'Variable', 'Note'])

df_final.head()

df_final= df_final.rename(columns={"Value (Thousands of Passengers)":"Annual No. of Passengers (Thousand)"})
df_final.head()

df_final.shape

df_final.sort_values(["City"])

df_final.shape

df_final2 = pd.read_csv("ManualData.csv")

df_final2.sort_values(["City"])

df_final2

mergedf1 = pd.merge(df_final, df_final2, on = ["City", "Year"])
mergedf1.head()

mergedf1

mergedf1.set_index('Year')

## Exploratory Data Analysis
mergedf1.describe(include=object)
mergedf1.describe()

mergedf1["City"].unique()

duplicate_rows_df = mergedf1[mergedf1.duplicated()] 
print("Number of duplicate rows: ", duplicate_rows_df.shape)

print(mergedf1.isnull().sum())

## use of fillna() to take average of previous four years for each city to fill null value with mean of previous four years. Source: https://stackoverflow.com/questions/58507317/how-to-fillna-impute-by-using-the-mean-of-the-last-3-rows-in-the-same-column

mergedf1.fillna(mergedf1.rolling(4, min_periods=1).mean().shift()) 

## using a boxplot to gain insight into the data 

sns.boxplot(x=mergedf1['Annual No. of Passengers (Thousand)']) 

plt.title("Annual No. of Passengers Across Seven European Cities", fontsize=14)
plt.xlabel("No. of Passengers (Thousand)", fontsize=12)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Annual No. of Passengers (Thousand)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Annual Number of Passengers Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Population (thousand)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Population Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Congestion (%)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Congestion Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

sns.scatterplot(data=mergedf1, x="Annual No. of Passengers (Thousand)", y="Congestion (%)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Annual Number of Passengers vs Congestion (%)", fontsize=14)
plt.xlabel("Annual No. of Passengers (Thousand)", fontsize=10)
plt.ylabel("Congestion (%)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Price of One Litre of Diesel (Euro)" , y="Congestion (%)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Price of One Litre of Diesel (Euro) vs Congestion (%)", fontsize=14)
plt.xlabel("Price of One Litre of Diesel (Euro)", fontsize=10)
plt.ylabel("Congestion (%)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Annual No. of Passengers (Thousand)", y="Population (thousand)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Annual No. of Passengers (Thousand) vs Population (thousand)", fontsize=14)
plt.xlabel("Annual No. of Passengers (Thousand)", fontsize=10)
plt.ylabel("Population (thousand)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Congestion (%)", y="Population (thousand)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Congestion (%) vs Population (thousand)", fontsize=14)
plt.xlabel("Congestion (%)", fontsize=10)
plt.ylabel("Population (thousand)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

mergedf1.corr()

## plotting data to gain insights into the data set

sns.relplot(data=mergedf1, x="Year", y="Annual No. of Passengers (Thousand)", hue="City", kind="line", height=8)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-") ## Source https://www.geeksforgeeks.org/grids-in-matplotlib/

plt.title("Annual No. of Passengers Across Seven European Cities", fontsize=14)
plt.xlabel("Year", fontsize=12)
plt.ylabel("Annual No. of Passengers (thousand)", fontsize=12)

sns.histplot(data=mergedf1, x="Congestion (%)", kde=True)

sns.histplot(data=mergedf1, x="Price of One Litre of Diesel (Euro)", kde=True)

# Statistical Analysis

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import statistics as stats
import scipy.stats as stats
from statsmodels.stats import weightstats

### Descriptive Statistics
mergedf.describe()

sns.pairplot(mergedf)
plt.grid(True, color = "grey", linewidth = "1", linestyle = "-") ## Source https://www.geeksforgeeks.org/grids-in-matplotlib/
plt.title("Graphs of Various Variables Effecting Congestion in Cities in Europe", fontsize=14)

sns.histplot(data=mergedf, x="Congestion (%)", kde=True)

sns.histplot(data=mergedf, x="Annual No. of Passengers (Thousand)", kde=True)

sns.histplot(data=mergedf, x="Population", kde=True)

plt.title("Histogram Showing Population of Seven European Cities 2018-2022", fontsize=14)

### Inferential Statistics

## Hypothesis Test (Z-Test)
X = mergedf["Annual No. of Passengers (Thousand)"]
passenger_std = X.std()
#### H0: mu = 58071, H1: mu > 58071 : As the mean of the annual number of passengers is 58071 the alternative hypothesis should be greater than 5000.
mu = 58071
z_score, p_value = weightstats.ztest(X, value = mu, ddof=1, alternative = 'larger')
z_score
p_value
As the p-value is greater than alpha, we reject the null hypothesis and accept the alternative hypothesis that mu > 58071. 
Therefore, we have enough evidence at 95% confidence interval to suggest that the average annual number of passengers will be greater than 58,071,000 across the seven cities.
X = mergedf["Congestion"]
congestion_std1 = X.std()
## H0: mu = 29.42, H1: mu > 29.42 : As the mean of the annual number of passengers is 58071 the alternative hypothesis should be greater than 5000.
mu = 29.42
z_score, p_value = weightstats.ztest(X, value = mu, ddof=1, alternative = 'larger')
z_score
p_value
As the p-value is greater than alpha we accept the null hypothesis and reject the alternative hypothesis that mu > 29.42 
Therefore, we have enough evidence at 95% confidence interval to suggest that the average congestion level across the seven cities will be 29.42

## Confidence Intervals
## define x (variable to analyse)- Annual No. Of Passengers
X = mergedf.iloc[:,2:3].values
## create our confidence interval using Student T's at 95% confidence interval
stats.t.interval(confidence = 0.95, df=len(X), loc=np.mean(X), scale = stats.sem(X))
At 95% confidence level, the annual number of passengers in the seven countries considered will be between 36438.70 and 79704.77 thousand people. 

## define x (variable to analyse)- Congestion (%)
X1 = mergedf.iloc[:,3:4].values
## create our confidence interval using Student T's at 95% confidence interval
stats.t.interval(confidence = 0.95, df=len(X1), loc=np.mean(X1), scale = stats.sem(X1))
At 95% confidence level, the average congestion level across the seven countries considered will be between 26.75% and 32.11% people.

## Normal Distribution
from scipy import stats
stats.norm.interval(0.95, loc=np.mean(X), scale=stats.sem(X))
Values for normal distribution are very similar to that of the Student T's

# Parametric Testing
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
import scipy as scipy

## Correlation Analysis
corr_main = mergedf.drop(columns=["City","Year"])
corr_main.corr()
corr = corr_main.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Variables from Seven European Cities", fontsize=18, loc="center")

mergedf = mergedf.rename(columns={"Annual No. of Passengers (Thousand)":"Passengers", "Population (thousand)":"Population", "Congestion (%)":"Congestion"})
pivot1new = df1.pivot(index="Year", columns="City", values="Passengers")
pivot1new
pivot1new.corr()
corr = pivot1new.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Annual No. of Passengers from Seven European Cities", fontsize=18, loc="center")

pivot2 = mergedf.drop(columns=["Passengers", "Population", "Price of One Litre of Diesel (Euro)"])
pivot2new = pivot2.pivot(index="Year", columns="City", values="Congestion")
pivot2new
pivot2new.corr()
corr = pivot2new.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Congestion Levels from Seven European Cities", fontsize=18, loc="center")

## ANOVA 
### Sharpiro Wilk Test- Checking the conditions for ANOVA
#QQ-plot
stats.probplot(mergedf.Passengers, plot = plt)
plt.figure()
## Sharpiro Wil Test to test Normality of Data
## H0: Data comes from normal dist
## H1: Data does NOT come from normal dist
stats.shapiro(mergedf.Passengers[mergedf.City=="Dublin"])
## p_value is greater than alpha therefore null hypothesis should be accepted.
## Evidence suggests that data for Dublin comes from normal distribution
stats.shapiro(mergedf.Passengers[mergedf.City=="Copenhagen"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Porto"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Zurich"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Glasgow"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Edinburgh"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Manchester"])
## As evidenced above by plot and p_value test, data for Zurich is NOT normally distributed

#QQ-plot
stats.probplot(mergedf.Congestion, plot = plt)
plt.figure()
stats.shapiro(mergedf.Congestion[mergedf.City=="Dublin"])
## p_value is greater than alpha therefore null hypothesis should be accepted.
## Evidence suggests that data for Dublin comes from normal distribution
stats.shapiro(mergedf.Congestion[mergedf.City=="Copenhagen"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Porto"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Zurich"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Manchester"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Edinburgh"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Glasgow"])
### Preparing the variables
dublin = mergedf.Congestion[mergedf.City=="Dublin"]
copenhagen = mergedf.Congestion[mergedf.City=="Copenhagen"]
porto = mergedf.Congestion[mergedf.City=="Porto"]
zurich = mergedf.Congestion[mergedf.City=="Zurich"]
manchester = mergedf.Congestion[mergedf.City=="Manchester"]
glasgow = mergedf.Congestion[mergedf.City=="Glasgow"]
edinburgh = mergedf.Congestion[mergedf.City=="Edinburgh"]

## Homogenity of the variances ---> Levene Test
## H0: the variances are equal
## H1: the variances are not equal
from scipy.stats import levene
## Levene Test for Homogeinety
levene(dublin, copenhagen, porto, zurich, edinburgh, manchester, glasgow, center = 'mean')
## H0: muDublin = muCopenhgen = muZurich = muPorto = muManchester = muGlasgow = muEdinburgh
## H1: there are at least 2 mu which are different

## we analyse congestion based on city
## ANOVA (One Way)
model = ols('Congestion~City', data=mergedf).fit()
aov = sm.stats.anova_lm(model, type=2)
print(aov)
## therefore as PR(>F) value which is our p_value is less than alpha then we reject the null hypothesis.

## ANOVA (Two Way)
## H0: means are equal
## H1: at least one mean is different
model2 = ols('Congestion~Year+City', data = mergedf).fit()
aov2 = sm.stats.anova_lm(model2, type = 2)
print(aov2)
## therefore as PR(>F) value which is our p_value is less than alpha then we reject the null hypothesis.
## Means are not equal based on City and Year of Congestion Data.

# Non-Parametric Tests
### Kruskal Wallis Test
new_mergedf_pivot
## Step One: Hypothesis

## H0: there is no difference between the numbers of passengers in each city
## H1: there is at least one city that has a different level of passengers

## Step Two: Create Variables
C= pivot1new["Copenhagen"]
D= pivot1new["Dublin"]
E= pivot1new["Edinburgh"]
G= pivot1new["Glasgow"]
M= pivot1new["Manchester"]
P= pivot1new["Porto"]
Z= pivot1new["Zurich"]
from scipy.stats import kruskal
stat, p = kruskal(C,D,E,G,M,P,Z)
print("Statistical Test: ", stat)
print("p-value= ", p)
## Therefore, p-value is less than alpha so we reject the null hypothesis.
## Evidence suggests that there is at least one city that has a different level of passenger

## Step One: Hypothesis
## H0: there is no difference between the congestion level in each city
## H1: there is at least one city that has a different congestion level
## Step Two: Create Variables
C1= pivot2new["Copenhagen"]
D1= pivot2new["Dublin"]
E1= pivot2new["Edinburgh"]
G1= pivot2new["Glasgow"]
M1= pivot2new["Manchester"]
P1= pivot2new["Porto"]
Z1= pivot2new["Zurich"]
stat1, p1 = kruskal(C1,D1,E1,G1,M1,P1,Z1)
print("Statistical Test: ", stat1)
print("p-value= ", p1)
## Therefore, p-value is less than alpha so we reject the null hypothesis.
## Evidence suggests that there is at least one city that has a different level of congestion

## U-Mann Whitman Test
##Step One: Hypothesis
## H0: there is no difference between the passenger numbers in cities
## H1: there is a difference between the passenger numbers in cities
## Step Two: Creating Variables
## Step 3: Test
from scipy.stats import mannwhitneyu
stat, p = mannwhitneyu(D,C)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,P)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,Z)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,G)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,E)
print ("Statistical Test: ", stat)
print("p-value: ", p)

##Step One: Hypothesis

## H0: there is no difference between the congestion levels in cities
## H1: there is a difference between the congestion levels in cities
## Step Two: Creating Variables
## Step 3: Test
stat, p = mannwhitneyu(D1,C1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,M1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,G1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,E1)
print ("Statistical Test: ", stat)
print("p-value: ", p)


# Machine Learning
## Linear Regression-Congestion Data
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import math
df = pd.read_csv("Congestion.csv")
df1 = pd.read_csv("Passengers.csv")
df2 = pd.read_csv("ManualDataCA2.csv")

import pandas as pd
from sklearn.linear_model import LinearRegression
##  year as index 
data = df.set_index('Year')
## Create Linear Regression model
model = LinearRegression()
## feature extraction - target variable passsengers nums
X = data.index.values.reshape(-1, 1)  
y = data.values  
## Fit the model
model.fit(X, y)
## predicting congestion for a given year
year1 = 2023
predicted_congestion = model.predict([[year1]])
print(f"Predicted congestion for the year {year1}: {predicted_congestion}")
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
y_pred = model.predict(X)
# Calculate R2 score
r2 = r2_score(y, y_pred)
print(f"R2 Score: {r2}")
y_pred = model.predict(X)
# Calculate Mean Squared Error
mse = mean_squared_error(y, y_pred)
print(f"Mean Squared Error: {mse}")

## Linear Regression-Passengert Data
### https://realpython.com/linear-regression-in-python/
import pandas as pd
from sklearn.linear_model import LinearRegression
##  year as index 
data = df1.set_index('Year')
## Create Linear Regression model
model = LinearRegression()
## feature extraction - target variable passsengers nums
X = data.index.values.reshape(-1, 1) 
y = data.values  
## Fit the model
model.fit(X, y)
## predicting passngers no. for a given year
year = 2023
predicted_passengers = model.predict([[year]])
print("Predicted passenger for the year {year}: {predicted_passengers}")
y_pred = model.predict(X)
## R2 score
r2 = r2_score(y, y_pred)
print(f"R2 Score: {r2}")
y_pred = model.predict(X)
## Mean Squared Error
mse = mean_squared_error(y, y_pred)
print(f"Mean Squared Error: {mse}")

kNN Regressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error 
sns.scatterplot(x=df2['Congestion (%)'], y=df2['Annual No. of Passengers (Thousand)'], hue=df2["City"])
# Create the independent and dependent variables
x = df2[['Congestion (%)']]
y = df2[['Annual No. of Passengers (Thousand)']]
from sklearn.neighbors import KNeighborsRegressor

# Declare arrays to store training and test accuracies
neighbors = np.arange(1, 9)
train_accuracy =np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
for i, k in enumerate(neighbors):
    # Declare and initialise kNN regressor with k neighbors
    kNN = KNeighborsRegressor(n_neighbors = k)
    # Call the method fit() to train the model
    kNN.fit(x_train, y_train)
    # Compute accuracy on the training set
    train_accuracy[i] = kNN.score(x_train, y_train)
    # Compute accuracy on the test set
    test_accuracy[i] = kNN.score(x_test, y_test) 
# Visualise the accuracy based on the number of neighbors
plt.title('kNN considering different neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.show()
print("Training set score: {:.2f}".format(kNN.score(x_train, y_train)))
print("Test set score: {:.2f}".format(kNN.score(x_test, y_test)))
## calculating MSE Source: https://www.datatechnotes.com/2019/04/regression-example-with-k-nearest.html
pred_y = kNN.predict(x)
mse = mean_squared_error(y, pred_y)
print("Mean Squared Error:",mse)

## Decision Tree Model
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
import statistics as stats
import scipy.stats as stats
from statsmodels.stats import weightstats

# split dataset in features and target variable
feature_cols = ['Year', 'Annual No. of Passengers (Thousand)', 'Congestion (%)','Price of One Litre of Diesel (Euro)','Population (thousand)']
X = df2[feature_cols] # Features
y = df2.City # Target variable
X, y
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test
# Create Decision Tree classifer object
clf = DecisionTreeClassifier(max_depth = 3, random_state = 0)
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
#Predict the response for test dataset
y_pred = clf.predict(X_test)
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
# column names for dataset
fn=['Year', 'Passengers', 'Congestion','Price of One Litre of Diesel (Euro)','Population']
# classes of dataset
cn=['0','1','2','3','4','5','6']
# Setting dpi = 300 to make image clearer than default
fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)
tree.plot_tree(clf,
           feature_names = fn, 
           class_names=cn,
           filled = True);
# In case of any errors, install conda install python-graphviz     on the command line
### Optimizing Decision Tree Performance
# Create Decision Tree classifer object
clf = DecisionTreeClassifier(criterion="entropy", max_depth=3)

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# column names for dataset
fn=['Year', 'Passengers', 'Congestion','Price of One Litre of Diesel (Euro)','Population']
# classes of dataset
cn=['0','1','2','3','4','5','6']

# Setting dpi = 300 to make image clearer than default
fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)

tree.plot_tree(clf,
           feature_names = fn, 
           class_names=cn,
           filled = True);
# set the path of your system
fig.savefig('imagename.png',bbox_inches='tight')
## Principal Component Analysis
df_PCA = df2.drop(columns=["Year", "City", "Population (thousand)", "Price of One Litre of Diesel (Euro)"])
df_PCA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
# Importing standardscalar module 
from sklearn.preprocessing import StandardScaler
scalar = StandardScaler()
# fitting
scalar.fit(df_PCA)
scaled_data = scalar.transform(df_PCA)
# Importing PCA
from sklearn.decomposition import PCA
# Let's say, components = 2
pca = PCA(n_components = 2)
pca.fit(scaled_data)
x_pca = pca.transform(scaled_data)
x_pca.shape
# giving a larger plot
plt.figure(figsize =(8, 6))
plt.scatter(x_pca[:, 0], x_pca[:, 1], c = df_PCA['Congestion (%)'], cmap ='plasma')
# labeling x and y axes
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')

# Sentiment Analysis
## VADER sentiment analysis with NLTK
import warnings
warnings.filterwarnings('ignore') # We can suppress the warnings
pip install vaderSentiment
pip install PyPDF2 nltk
## Merged Transport Survey Document 20222
import PyPDF2
import nltk
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from nltk.tokenize import sent_tokenize
nltk.download('punkt') ## source:https://realpython.com/python-nltk-sentiment-analysis/
## https://vadersentiment.readthedocs.io/en/latest/pages/code_and_example.html
## https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/
pdf_text = extract_text_from_pdf('Merged Transport Survey Doc 2022.pdf')
sentiment = SentimentIntensityAnalyzer()
sent_1 = sentiment.polarity_scores(pdf_text)
print("Sentiment of pdf:", sent_1)
sentences = sent_tokenize(pdf_text)
sentiment = SentimentIntensityAnalyzer()
for sentence in sentences:
    vs = sentiment.polarity_scores(sentence)
    print("{:-<65} {}".format(sentence, str(vs)))
## Customer Satisfaction Survey 2022
## https://vadersentiment.readthedocs.io/en/latest/pages/code_and_example.html
## ## https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/
pdf_text1 = extract_text_from_pdf('Customer_Satisfaction_Survey_2022.pdf')
sentiment1 = SentimentIntensityAnalyzer()
sent_2 = sentiment1.polarity_scores(pdf_text1)
print("Sentiment of pdf:", sent_2)
sentiment1 = SentimentIntensityAnalyzer()
for sentence in pdf_text1:
    vs = sentiment1.polarity_scores(sentence)
    print("{:-<65} {}".format(sentence, str(vs)))

# Interactive Dashboard
df = pd.read_csv("ManualDataCA2.csv")
congest = df[['Year', 'City', 'Congestion (%)']]
passenger = df[['Year', 'City', 'Annual No. of Passengers (Thousand)']]
conpass = df[['Congestion (%)', 'Annual No. of Passengers (Thousand)', 'City', 'Year']]
### prepare colors for use throughout in styling the dashboard
colors = {
    'background': '#000000',
    'text': '#FFFFFF'
}
### create the instance of the Dash class 
app = dash.Dash(__name__)
### create the app layout- set background colours

app.layout = html.Div(style={'backgroundColor': colors['background']}, children=[

    html.H1("LIGHT RAIL AND TRAM SERVICES DATA FOR EUROPEAN CITIES", style={'text-align': 'center', 'color': colors['text']}),
    
    ### create the dropdown menu for selecting each city
    dcc.Dropdown(
        id="dropdown",
        options=["Dublin", "Copenhagen", "Zurich", "Porto", "Glasgow", "Manchester", "Edinburgh", "All Cities"],
        value="All Cities",
        clearable=False,
        style={'width': "50%",'height':'30px','padding-left':'2%','display':'inline-block'}),
    
    ### create the dropdown menu for selecting which graph to view
    dcc.Dropdown(
        id="dropdown2",
        options=["Congestion", "Annual Passenger Numbers", "Congestion vs Passenger Numbers"],
        value="Select Data:",
        clearable=False,
        style={'width': "50%",'height':'30px','padding-left':'2%','display':'inline-block'}),
    
    ### needed to create a new .Div for the output graph as I could not get the area surrounding it to change colour otherwise
    html.Div(style={'backgroundColor': colors['background'],'marginLeft':'2%','marginRight':'2%'}, children=[
        dcc.Graph(
            id="graph",
            style={'backgroundColor': colors['background']}) ### set colour for the background
])
])
### Connect the Plotly graphs with Dash Components
@app.callback(
    Output("graph", "figure"), 
    Input("dropdown", "value"), ### for the dropdown menu to select city
    Input("dropdown2", "value"))### for the dropdown menu to select what graph to view

### callback function used to update the graph with the required data
def update_bar_chart(city, slct_var):
    if city == "All Cities":  ### used an if-else statement to handle the selection of all cities or the selection of a specific cityto view data from all cities on one graph
        if slct_var == "Congestion": ### when the Congestion variable was selected to view using an if-else statement withinthe main if-else statement
            df = congest  ### using the congest dataframe created at the beginning
            y_column = "Congestion (%)"
            title = "Congestion for All Cities from 2018-2022" 
            x_title = "Year"
            y_title = "Congestion (%)"
            fig = px.bar(df, x="Year", y=y_column, color="City", barmode="group")
            fig.update_layout(title=title, xaxis_title=x_title, yaxis_title=y_title, paper_bgcolor='#000000', plot_bgcolor='#000000',  font_color="#FFFFFF", title_font_color="#FFFFFF",)            
            return fig
        elif slct_var == "Annual Passenger Numbers": ### when the Passengers variable was selected to view
            df = passenger  
            y_column = "Annual No. of Passengers (Thousand)"
            title = "Passenger Numbers for All Cities from 2018-2022"
            x_title = "Year"
            y_title = "Passengers (Thousand)"
            fig = px.bar(df, x="Year", y=y_column, color="City", barmode="group")
            fig.update_layout(title=title, xaxis_title=x_title, yaxis_title=y_title, paper_bgcolor='#000000', plot_bgcolor='#000000',  font_color="#FFFFFF", title_font_color="#FFFFFF")
            return fig
        elif slct_var == "Congestion vs Passenger Numbers": ### when the Passengers vs Congestion graph was selected to view
            df = conpass  
            x_column = "Annual No. of Passengers (Thousand)"
            y_column = "Congestion (%)"  # Assuming this column exists in the passenger dataset
            x_title = "Passenger Numbers"
            y_title = "Congestion (%)"
            fig = px.scatter(df, x=x_column, y=y_column, color='City', hover_data=["Year", "City", "Annual No. of Passengers (Thousand)", y_column])
            fig.update_layout(title=f"Congestion vs Passenger Numbers for All Cities from 2018-2022", xaxis_title="Passenger Numbers", yaxis_title="Congestion (%)", paper_bgcolor='#000000', plot_bgcolor='#000000',  font_color="#FFFFFF", title_font_color="#FFFFFF")
            return fig
        else:
            return {}
    else: ### else if a specific city is selected form the dropdown menu for cities
        
        if slct_var == "Congestion": ### if-else statement within the main statement 
            df = congest
            y_column = "Congestion (%)" 
            title = f"Congestion in {city} from 2018-2022"
            x_title = "Year"
            y_title = "Congestion (%)"
            mask = df["City"] == city
            fig = px.bar(df[mask], x="Year", y=y_column, barmode="group")
            fig.update_layout(title=title, xaxis_title=x_title, yaxis_title=y_title, paper_bgcolor='#000000', plot_bgcolor='#000000',  font_color="#FFFFFF", title_font_color="#FFFFFF")
            return fig
        elif slct_var == "Annual Passenger Numbers":
            df = passenger  
            y_column = "Annual No. of Passengers (Thousand)"  
            title = f"Passenger Numbers in {city} from 2018-2022"
            x_title = "Year"
            y_title = "Passengers (Thousand)"
            mask = df["City"] == city
            fig = px.bar(df[mask], x="Year", y=y_column, barmode="group")
            fig.update_layout(title=title, xaxis_title=x_title, yaxis_title=y_title, paper_bgcolor='#000000', plot_bgcolor='#000000',  font_color="#FFFFFF", title_font_color="#FFFFFF")
            return fig
        elif slct_var == "Congestion vs Passenger Numbers":
            df = conpass  
            x_column = "Annual No. of Passengers (Thousand)"
            y_column = "Congestion (%)" 
            x_title = "Passenger Numbers"
            y_title = "Congestion (%)"
            fig = px.scatter(df[df["City"] == city], x=x_column, y=y_column, hover_data=["Year", "City", "Annual No. of Passengers (Thousand)", y_column])
            fig.update_layout(title=f"Congestion vs Passenger Numbers in {city} from 2018-2022", xaxis_title="Passenger Numbers", yaxis_title="Congestion (%)", paper_bgcolor='#000000', plot_bgcolor='#000000',  font_color="#FFFFFF", title_font_color="#FFFFFF")
            return fig
        else:
            return {}
if __name__ == '__main__':
    app.run_server(debug=True)



