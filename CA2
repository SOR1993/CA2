# MSc Data Analytics CA2
## Data Visualisation and Preparation

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import math

## Data Extraction and Manipulation
df_UN = pd.read_csv("UNDataset.csv")

df_UN.head()

df_UN["City"].unique()

df_UN.sort_values(["City"])

newdf = df_UN.loc[(df_UN["City"] == 'Dublin') | 
                  (df_UN["City"] == 'Porto') | 
                  (df_UN["City"] == 'Glasgow') | 
                  (df_UN["City"] == 'Manchester')| 
                  (df_UN["City"] == 'Edinburgh')| 
                  (df_UN["City"] == 'Copenhagen')|
                 (df_UN["City"] == 'Zurich')]
newdf

newdf["City"].unique()

newdf1 = newdf.loc[(newdf["Type"] == 'Tram')|
                 (newdf["Type"] == 'Metro')]
newdf1

newdf2 = newdf1.loc[(newdf1["Variable"] == 'Pass')]

newdf3 = newdf2.loc[(newdf2["Year"] >= 2018)]

newdf3.shape

df_final=newdf3.drop(columns=['Countrycode','Type', 'Variable', 'Note'])

df_final.head()

df_final= df_final.rename(columns={"Value (Thousands of Passengers)":"Annual No. of Passengers (Thousand)"})
df_final.head()

df_final.shape

df_final.sort_values(["City"])

df_final.shape

df_final2 = pd.read_csv("ManualData.csv")

df_final2.sort_values(["City"])

df_final2

mergedf1 = pd.merge(df_final, df_final2, on = ["City", "Year"])
mergedf1.head()

mergedf1

mergedf1.set_index('Year')

## Exploratory Data Analysis
mergedf1.describe(include=object)
mergedf1.describe()

mergedf1["City"].unique()

duplicate_rows_df = mergedf1[mergedf1.duplicated()] 
print("Number of duplicate rows: ", duplicate_rows_df.shape)

print(mergedf1.isnull().sum())

## use of fillna() to take average of previous four years for each city to fill null value with mean of previous four years. Source: https://stackoverflow.com/questions/58507317/how-to-fillna-impute-by-using-the-mean-of-the-last-3-rows-in-the-same-column

mergedf1.fillna(mergedf1.rolling(4, min_periods=1).mean().shift()) 

## using a boxplot to gain insight into the data 

sns.boxplot(x=mergedf1['Annual No. of Passengers (Thousand)']) 

plt.title("Annual No. of Passengers Across Seven European Cities", fontsize=14)
plt.xlabel("No. of Passengers (Thousand)", fontsize=12)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Annual No. of Passengers (Thousand)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Annual Number of Passengers Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Population (thousand)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Population Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

## plotting data to gain insights into the data set

fig, ax = plt.subplots(figsize=(30, 15))
sns.barplot(mergedf1, x="Year", y="Congestion (%)", hue="City", width=.9)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")
plt.legend(fontsize="22", loc ="upper left")

plt.title("Congestion Across Seven European Cities", fontsize=36)
plt.xlabel("Year", fontsize=28)
plt.ylabel("Number of Passengers (Thousand)", fontsize=28)
plt.xticks(fontsize=20) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=20)

sns.scatterplot(data=mergedf1, x="Annual No. of Passengers (Thousand)", y="Congestion (%)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Annual Number of Passengers vs Congestion (%)", fontsize=14)
plt.xlabel("Annual No. of Passengers (Thousand)", fontsize=10)
plt.ylabel("Congestion (%)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Price of One Litre of Diesel (Euro)" , y="Congestion (%)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Price of One Litre of Diesel (Euro) vs Congestion (%)", fontsize=14)
plt.xlabel("Price of One Litre of Diesel (Euro)", fontsize=10)
plt.ylabel("Congestion (%)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Annual No. of Passengers (Thousand)", y="Population (thousand)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Annual No. of Passengers (Thousand) vs Population (thousand)", fontsize=14)
plt.xlabel("Annual No. of Passengers (Thousand)", fontsize=10)
plt.ylabel("Population (thousand)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

sns.scatterplot(data=mergedf1, x="Congestion (%)", y="Population (thousand)", hue="City")

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-")

plt.title("Congestion (%) vs Population (thousand)", fontsize=14)
plt.xlabel("Congestion (%)", fontsize=10)
plt.ylabel("Population (thousand)", fontsize=10)
plt.xticks(fontsize=10) ## https://stackabuse.com/rotate-axis-labels-in-matplotlib/
plt.yticks(fontsize=10)

mergedf1.corr()

## plotting data to gain insights into the data set

sns.relplot(data=mergedf1, x="Year", y="Annual No. of Passengers (Thousand)", hue="City", kind="line", height=8)

plt.grid(True, color = "grey", linewidth = "1", linestyle = "-") ## Source https://www.geeksforgeeks.org/grids-in-matplotlib/

plt.title("Annual No. of Passengers Across Seven European Cities", fontsize=14)
plt.xlabel("Year", fontsize=12)
plt.ylabel("Annual No. of Passengers (thousand)", fontsize=12)

sns.histplot(data=mergedf1, x="Congestion (%)", kde=True)

sns.histplot(data=mergedf1, x="Price of One Litre of Diesel (Euro)", kde=True)

# Statistical Analysis

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import statistics as stats
import scipy.stats as stats
from statsmodels.stats import weightstats

### Descriptive Statistics
mergedf.describe()

sns.pairplot(mergedf)
plt.grid(True, color = "grey", linewidth = "1", linestyle = "-") ## Source https://www.geeksforgeeks.org/grids-in-matplotlib/
plt.title("Graphs of Various Variables Effecting Congestion in Cities in Europe", fontsize=14)

sns.histplot(data=mergedf, x="Congestion (%)", kde=True)

sns.histplot(data=mergedf, x="Annual No. of Passengers (Thousand)", kde=True)

sns.histplot(data=mergedf, x="Population", kde=True)

plt.title("Histogram Showing Population of Seven European Cities 2018-2022", fontsize=14)

### Inferential Statistics

## Hypothesis Test (Z-Test)
X = mergedf["Annual No. of Passengers (Thousand)"]
passenger_std = X.std()
#### H0: mu = 58071, H1: mu > 58071 : As the mean of the annual number of passengers is 58071 the alternative hypothesis should be greater than 5000.
mu = 58071
z_score, p_value = weightstats.ztest(X, value = mu, ddof=1, alternative = 'larger')
z_score
p_value
As the p-value is greater than alpha, we reject the null hypothesis and accept the alternative hypothesis that mu > 58071. 
Therefore, we have enough evidence at 95% confidence interval to suggest that the average annual number of passengers will be greater than 58,071,000 across the seven cities.
X = mergedf["Congestion"]
congestion_std1 = X.std()
## H0: mu = 29.42, H1: mu > 29.42 : As the mean of the annual number of passengers is 58071 the alternative hypothesis should be greater than 5000.
mu = 29.42
z_score, p_value = weightstats.ztest(X, value = mu, ddof=1, alternative = 'larger')
z_score
p_value
As the p-value is greater than alpha we accept the null hypothesis and reject the alternative hypothesis that mu > 29.42 
Therefore, we have enough evidence at 95% confidence interval to suggest that the average congestion level across the seven cities will be 29.42

## Confidence Intervals
## define x (variable to analyse)- Annual No. Of Passengers
X = mergedf.iloc[:,2:3].values
## create our confidence interval using Student T's at 95% confidence interval
stats.t.interval(confidence = 0.95, df=len(X), loc=np.mean(X), scale = stats.sem(X))
At 95% confidence level, the annual number of passengers in the seven countries considered will be between 36438.70 and 79704.77 thousand people. 

## define x (variable to analyse)- Congestion (%)
X1 = mergedf.iloc[:,3:4].values
## create our confidence interval using Student T's at 95% confidence interval
stats.t.interval(confidence = 0.95, df=len(X1), loc=np.mean(X1), scale = stats.sem(X1))
At 95% confidence level, the average congestion level across the seven countries considered will be between 26.75% and 32.11% people.

## Normal Distribution
from scipy import stats
stats.norm.interval(0.95, loc=np.mean(X), scale=stats.sem(X))
Values for normal distribution are very similar to that of the Student T's

# Parametric Testing
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
import scipy as scipy

## Correlation Analysis
corr_main = mergedf.drop(columns=["City","Year"])
corr_main.corr()
corr = corr_main.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Variables from Seven European Cities", fontsize=18, loc="center")

mergedf = mergedf.rename(columns={"Annual No. of Passengers (Thousand)":"Passengers", "Population (thousand)":"Population", "Congestion (%)":"Congestion"})
pivot1new = df1.pivot(index="Year", columns="City", values="Passengers")
pivot1new
pivot1new.corr()
corr = pivot1new.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Annual No. of Passengers from Seven European Cities", fontsize=18, loc="center")

pivot2new
pivot2new.corr()
corr = pivot2new.corr()
sns.heatmap(corr, cmap="Blues", annot=True)
plt.title("Correlations of Congestion Levels from Seven European Cities", fontsize=18, loc="center")

## ANOVA 
### Sharpiro Wilk Test- Checking the conditions for ANOVA
#QQ-plot
stats.probplot(mergedf.Passengers, plot = plt)
plt.figure()
## Sharpiro Wil Test to test Normality of Data
## H0: Data comes from normal dist
## H1: Data does NOT come from normal dist
stats.shapiro(mergedf.Passengers[mergedf.City=="Dublin"])
## p_value is greater than alpha therefore null hypothesis should be accepted.
## Evidence suggests that data for Dublin comes from normal distribution
stats.shapiro(mergedf.Passengers[mergedf.City=="Copenhagen"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Porto"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Zurich"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Glasgow"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Edinburgh"])
stats.shapiro(mergedf.Passengers[mergedf.City=="Manchester"])
## As evidenced above by plot and p_value test, data for Zurich is NOT normally distributed

#QQ-plot
stats.probplot(mergedf.Congestion, plot = plt)
plt.figure()
stats.shapiro(mergedf.Congestion[mergedf.City=="Dublin"])
## p_value is greater than alpha therefore null hypothesis should be accepted.
## Evidence suggests that data for Dublin comes from normal distribution
stats.shapiro(mergedf.Congestion[mergedf.City=="Copenhagen"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Porto"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Zurich"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Manchester"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Edinburgh"])
stats.shapiro(mergedf.Congestion[mergedf.City=="Glasgow"])
### Preparing the variables
dublin = mergedf.Congestion[mergedf.City=="Dublin"]
copenhagen = mergedf.Congestion[mergedf.City=="Copenhagen"]
porto = mergedf.Congestion[mergedf.City=="Porto"]
zurich = mergedf.Congestion[mergedf.City=="Zurich"]
manchester = mergedf.Congestion[mergedf.City=="Manchester"]
glasgow = mergedf.Congestion[mergedf.City=="Glasgow"]
edinburgh = mergedf.Congestion[mergedf.City=="Edinburgh"]

## Homogenity of the variances ---> Levene Test
## H0: the variances are equal
## H1: the variances are not equal
from scipy.stats import levene
## Levene Test for Homogeinety
levene(dublin, copenhagen, porto, zurich, edinburgh, manchester, glasgow, center = 'mean')
## H0: muDublin = muCopenhgen = muZurich = muPorto = muManchester = muGlasgow = muEdinburgh
## H1: there are at least 2 mu which are different

## we analyse congestion based on city
## ANOVA (One Way)
model = ols('Congestion~City', data=mergedf).fit()
aov = sm.stats.anova_lm(model, type=2)
print(aov)
## therefore as PR(>F) value which is our p_value is less than alpha then we reject the null hypothesis.

## ANOVA (Two Way)
## H0: means are equal
## H1: at least one mean is different
model2 = ols('Congestion~Year+City', data = mergedf).fit()
aov2 = sm.stats.anova_lm(model2, type = 2)
print(aov2)
## therefore as PR(>F) value which is our p_value is less than alpha then we reject the null hypothesis.
## Means are not equal based on City and Year of Congestion Data.

# Non-Parametric Tests
### Kruskal Wallis Test
new_mergedf_pivot
## Step One: Hypothesis

## H0: there is no difference between the numbers of passengers in each city
## H1: there is at least one city that has a different level of passengers

## Step Two: Create Variables
C= new_mergedf_pivot["Copenhagen"]
D= new_mergedf_pivot["Dublin"]
E= new_mergedf_pivot["Edinburgh"]
G= new_mergedf_pivot["Glasgow"]
M= new_mergedf_pivot["Manchester"]
P= new_mergedf_pivot["Porto"]
Z= new_mergedf_pivot["Zurich"]
from scipy.stats import kruskal
stat, p = kruskal(C,D,E,G,M,P,Z)
print("Statistical Test: ", stat)
print("p-value= ", p)
## Therefore, p-value is less than alpha so we reject the null hypothesis.
## Evidence suggests that there is at least one city that has a different level of passenger

## Step One: Hypothesis
## H0: there is no difference between the congestion level in each city
## H1: there is at least one city that has a different congestion level
## Step Two: Create Variables
C1= pivot2new["Copenhagen"]
D1= pivot2new["Dublin"]
E1= pivot2new["Edinburgh"]
G1= pivot2new["Glasgow"]
M1= pivot2new["Manchester"]
P1= pivot2new["Porto"]
Z1= pivot2new["Zurich"]
stat1, p1 = kruskal(C1,D1,E1,G1,M1,P1,Z1)
print("Statistical Test: ", stat1)
print("p-value= ", p1)
## Therefore, p-value is less than alpha so we reject the null hypothesis.
## Evidence suggests that there is at least one city that has a different level of congestion

## U-Mann Whitman Test
##Step One: Hypothesis
## H0: there is no difference between the passenger numbers in cities
## H1: there is a difference between the passenger numbers in cities
## Step Two: Creating Variables
## Step 3: Test
from scipy.stats import mannwhitneyu
stat, p = mannwhitneyu(D,C)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,P)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,Z)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,G)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D,E)
print ("Statistical Test: ", stat)
print("p-value: ", p)

##Step One: Hypothesis

## H0: there is no difference between the congestion levels in cities
## H1: there is a difference between the congestion levels in cities
## Step Two: Creating Variables
## Step 3: Test
stat, p = mannwhitneyu(D1,C1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,M1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,G1)
print ("Statistical Test: ", stat)
print("p-value: ", p)
stat, p = mannwhitneyu(D1,E1)
print ("Statistical Test: ", stat)
print("p-value: ", p)






















